# -*- coding: utf-8 -*-
"""Data Structures package.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lf1VmlbgACOX5BZKb-5VKiwmzIuTV8Ka
"""

#WEB CRAWLING AND WEB SCRAPING 
          #PROJECT BY NILAVINI B.M 21PD22 & VARSHA S 21PD39


#modules that we have used
import requests as req
import urllib.request
from urllib.request import urljoin
from bs4 import BeautifulSoup
import requests
from urllib.request import urlparse
  
#queue ADT

class Queue(object):

  def _init_(self,size):
    self.queue=[]
    self.size=size

  def _str_(self):
    mystring = ' '.join(str(i) for i in self.queue)
    return mystring

  def enqueue(self,data):
    if(self.isfull()!=True):
      self.queue.append(data)
    else:
      print('Queue is FULL')

  def dequeue(self):
     if(self.isempty()!= True):
       return self.queue.pop(0)
     else:
       print('Queue is EMPTY')

  def isempty(self):
    return self.queue==[]

  def isfull(self):
    return len(self.queue)==self.size

  def front_data(self):
    if(self.isempty()!=True):
       return self.queue[0]
    else:
        print("Queue is EMPTY") 


# It is the list for storing urls with same domain 
links_intern = []

input_url = input("Enter the url : ") #the url we give
key=input("Enter your topic : ") #the topic we want to search in that website
depth = 1
  
# It is the list for storing urls with different domain
links_extern = []
suggested_url = []  #list to get the suggested urls from the given key


# Method for crawling a url at next level

#WEB CRAWLING

def web_crawler(input_url):
    
    temp_urls = [] 
    current_url_domain = urlparse(input_url).netloc
  
    # to extract html tags from the obj

    reqs=requests.get(input_url)
    b_soup= BeautifulSoup(reqs.content,"html.parser" )

    #crawling and segregating to input and output
    print("\n\n")
    print("-----------------------------------------------------------------------------------------------")
    print("-----------------------------------------CRAWLED WEB PAGES-------------------------------------")
    print(" -----------------------INTERNAL AND EXTERNAL URLS OF THE MAIN WEBSITE-------------------------")
    print("-----------------------------------------------------------------------------------------------")
    print("\n\n")

    for link in b_soup.findAll("a"):
        href = link.attrs.get("href")
        if(href != "" or href != None):
            href = urljoin(input_url, href)
            href_parsed = urlparse(href)
            href = href_parsed.scheme
            href += "://"
            href += href_parsed.netloc #from the location
            href += href_parsed.path
            final_parsed_href = urlparse(href)
            is_valid = bool(final_parsed_href.scheme) and bool(final_parsed_href.netloc)
            
            #condition 
            if is_valid:
                if current_url_domain not in href and href not in links_extern:
                    print("External - {}".format(href))
                    links_extern.append(href)
                if current_url_domain in href and href not in links_intern:
                    print("Internal - {}".format(href))
                    links_intern.append(href)
                    temp_urls.append(href)
                    if href.find(key)!=-1:
                        suggested_url.append(href)
    return temp_urls
  
  
if(depth == 1):
    web_crawler(input_url)
  
else:
    #breadth first search (bfs) method
    myqueue=Queue(1000)
    myqueue.enqueue(input_url)
    for j in range(depth):
        for count in range(len(myqueue)):
            url = myqueue.dequeue
            urls = web_crawler(url)
            for i in urls:
                myqueue.enqueue(i)
print("\n\n\n\n\n")
print("-------------------------------------------------------")
print("      SUGGESTED WEBPAGES FROM THE SAME DOMAIN")
print("-------------------------------------------------------")
print("\n\n\n")
for i in suggested_url:
  print(i)


#WEB SCRAPING 


req=requests.get(input("Enter the URL :"))
soup= BeautifulSoup(req.content,"html.parser" )

page_title=soup.title
tit= page_title.get_text()
print("\n\n")
print("---------------------------------------------------------------------------")
print("THE TITLE OF THE WEB PAGE IS :")
print("---------------------------------------------------------------------------")
print(tit)
print("---------------------------------------------------------------------------")
print("\n\n")


print("\n\n")
print("THE CONTENTS OF THE WEB PAGE IS :")
print("---------------------------------------------------------------------------")
content_page=soup.find_all('h2')
for course in content_page:
  print(course.text)
print("---------------------------------------------------------------------------")
print("\n\n")


print("\n\n")
print("THE MAIN CONTENTS OF THE GIVEN URL IS :")
print("---------------------------------------------------------------------------")
main_content_page=soup.find_all('h3')
for coursee in main_content_page:
  print(coursee.text)
print("---------------------------------------------------------------------------")
print("\n\n")